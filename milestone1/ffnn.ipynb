{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tugas Besar Machine Learning Milestone 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Fungsi-fungsi aktivasi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(Z):\n",
    "    c = 4\n",
    "    A = np.multiply(c, Z)\n",
    "    return A, Z\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A, Z\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    return A, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    e = np.exp(Z)\n",
    "    return e/e.sum(), Z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9525741268224334\n3\n12\n1.0\n"
     ]
    }
   ],
   "source": [
    "# testing activation function\n",
    "\n",
    "Z = [3, 7, 12]\n",
    "\n",
    "a, aa = sigmoid(3)\n",
    "b, bb = relu(3)\n",
    "c, cc = linear(3)\n",
    "d, dd = softmax(3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation_fn):\n",
    "\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "\n",
    "    if activation_fn == \"linear\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation_fn == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation_fn == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    elif activation_fn == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    else :\n",
    "        raise Exception(\"Activation Function unknown\")\n",
    "\n",
    "    assert A.shape == (W.shape[0], A_prev.shape[1])\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def L_model_forward(X, parameters, hidden_layers_activation_fn=\"relu\"):\n",
    "#     A = X                           \n",
    "#     caches = []                     \n",
    "#     L = len(parameters) // 2        \n",
    "\n",
    "#     for l in range(1, L):\n",
    "#         A_prev = A\n",
    "#         A, cache = linear_activation_forward(\n",
    "#             A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)],\n",
    "#             activation_fn=hidden_layers_activation_fn)\n",
    "#         caches.append(cache)\n",
    "\n",
    "#     AL, cache = linear_activation_forward(\n",
    "#         A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],\n",
    "#         activation_fn=\"sigmoid\")\n",
    "#     caches.append(cache)\n",
    "\n",
    "#     assert AL.shape == (1, X.shape[1])\n",
    "#     return AL, caches"
   ]
  },
  {
   "source": [
    "meh, lupakan ini..\n",
    "ternyata tugasnya bukan learning tapi baca model trus eksekusi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_parameters(layers_dims):\n",
    "#     np.random.seed(1)               \n",
    "#     parameters = {}\n",
    "#     L = len(layers_dims)            \n",
    "\n",
    "#     for l in range(1, L):           \n",
    "#         parameters[\"W\" + str(l)] = np.random.randn(\n",
    "#             layers_dims[l], layers_dims[l - 1]) * 0.01\n",
    "#         parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "#         assert parameters[\"W\" + str(l)].shape == (\n",
    "#             layers_dims[l], layers_dims[l - 1])\n",
    "#         assert parameters[\"b\" + str(l)].shape == (layers_dims[l], 1)\n",
    "    \n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "08876914b93de310088a782f60f6c6abc8e6f6e29f02ef0079734cfa98a8305c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tugas Besar Machine Learning Milestone 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Fungsi error"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Errorfunc(m,b):\n",
    "    global point\n",
    "    n = float(len(point))\n",
    "    err = 0\n",
    "    for p in point:\n",
    "        err += (p[0]-(m*p[1]+b))**2\n",
    "    return err/n"
   ]
  },
  {
   "source": [
    "## Fungsi-fungsi aktivasi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(c, Z):\n",
    "    A = np.multiply(c, Z)\n",
    "    return A\n",
    "\n",
    "def sigmoid(Z):\n",
    "    arr = []\n",
    "    for x in Z:\n",
    "        arr.append(1 / (1 + np.exp(-x)))\n",
    "    return arr\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    return A\n",
    "\n",
    "def softmax(Z):\n",
    "    e = np.exp(Z)\n",
    "    return e/e.sum()"
   ]
  },
  {
   "source": [
    "## Fungsi-fungsi untuk neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b #nilai sigma\n",
    "\n",
    "    if(len(Z) == 1):\n",
    "        return Z[0]\n",
    "\n",
    "    arr = []\n",
    "    for i in range(0,len(A_prev)):\n",
    "        arr.append(Z[i][i])\n",
    "\n",
    "    # print(arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation_fn):\n",
    "    Z = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation_fn == \"linear\":\n",
    "        A = linear(1, Z)\n",
    "\n",
    "    elif activation_fn == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "\n",
    "    elif activation_fn == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    elif activation_fn == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "    \n",
    "    else :\n",
    "        raise Exception(\"activation function unknown\")\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, hidden_layers_activation_fn=\"relu\"):\n",
    "    A = X                                               \n",
    "    L = len(parameters) // 2       \n",
    "\n",
    "    for l in range(1, L):\n",
    "        # print(l)\n",
    "        A_prev = A\n",
    "        A = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation_fn=hidden_layers_activation_fn)\n",
    "        # print(A)\n",
    "\n",
    "    AL = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],activation_fn=\"sigmoid\")\n",
    "\n",
    "    # assert AL.shape == (1, X.shape[1])\n",
    "    return AL"
   ]
  }
 ]
}